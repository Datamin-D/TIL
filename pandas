#1. map함수 쓰는법
df['column'].map(lambda x: x 에 할 연산) #column지정안하면 이상하게 나올 수 있음. series 용이라.
e.g.
#빈 리스트인지 확인
df['Genre'].map(lambda x : x == "[]") 
하면 series 값 별로 lambda함수를 연산해줌.

#2. DataFrame합치는 pd.concat()
#axis 방향
pd.concat(df1,df2,axis = 1) #옆으로 붙임
pd.concat(df1,df2,axis = 0) #밑에다 붙임.
first argument must be an iterable of pandas objects, you passed an object of type "DataFrame" 이 뜰것. 리스트로 넣어줘야하기 때문.

  #맞는 코드
  pd.concat([df1,df2],axis = 1) #옆으로 붙임
  pd.concat([df1,df2],axis = 0) #밑에다 붙임. 

# * pd.concat([],axis = 1)로 합칠 때 합치는 기준은 index임.  # 그냥 직관적으로 옆으로 갖다 붙이고 싶으면 reset index를 권함.
# 물론 pd.concat([df1,df2],ignore_index = True,axis = 1) 옵션을 주면 말그대로 옆으로 그냥 갖다붙임. 그러나 컬럼 이름이 0,1,2,..로 변해버림
# = pd.concat([df_5, df_6], keys=['df_5', 'df_6'], names=['df_name', 'row_number']) 같이 names = 를 통해 컬럼 이름을 주면 됨.



출처: https://rfriend.tistory.com/256 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]


#3. 열 삭제 df.drop()
df.drop(['컬럼1','컬럼2'],axis = 1) #axis 지정안해주면 컬럼 못찾겠다 에러날 수도.  
#이대로 놓는다고 df가 변하지는 않음.
df = df.drop(['컬럼1','컬럼2'],axis = 1) #까지 해줘야 함.

#4. 행삭제
df.drop([행인덱스,행인덱스])

5.행 수 세는법
len(df[df['Label'] == 'Benign']) 이렇게 len으로  특정 컬럼('Label')의 행 갯수 셀 수 있음.

6. 판다스 inplace의 의미
inplace 옵션에 True를 주면 또 다른 객체를 반환하지 않고 기존 객체를 수정 (ex. df.fillna(inplace=True) )
즉, 다시 df = 내가 원한 연산 이런거 안해줘도 됨.
출처: https://sacko.tistory.com/18 [데이터 분석하는 문과생, 싸코]

#7. Df에 새로운 행 추가.
팬더는 완전히 채워진 ‘데이터 프레임’을로드하도록 설계되었습니다.
그치만 append()메소드로 사전의 값을 행으로 직접 가져 와서 팬더dataframe에 추가 할 수 있습니다.
df = df.append({'컬럼':값},colimns = [],ignore_index = True)
https://www.delftstack.com/ko/howto/python-pandas/how-to-add-one-row-to-pandas-dataframe/

근데 위에를
temp_series = pd.Series()
df['컬럼'] = df['컬럼'].append(temp_series) #식으로 하면 에러남. Series라서. DataFrame으로 아래처럼 넣는게 좋음

temp_df = pd.DataFrame()
df = df.append(temp_df) #식으로 하면 에러남. Series라서. DataFrame으로 아래처럼 넣는게 좋음



#8. df에서 index를 새로운 컬럼으로 만들기.
df.reset_index() #하면됨.

#9. def안에서 df를 어쩔 수 없이 할당해야될때.

def scale_log(col_name):
  global df 
  # 밑에 drop을 사용하려면 필연적으로 df = 식 처럼 df에 할당해줘야함. 그런데 그러면 def에서 전역/지역변수 문제 발생.def안에 변수를 할당시키면 지역변수로 생각하기 때문.
  # global 해주면 말끔히 해결.
  
  log_col_name = 'log_' + col_name
  df[log_col_name] = df[col_name].apply(lambda x: np.log(x))
  df = df.drop(col_name, axis = 1) #이거때매그럼.
  return df[log_col_name]

#10. df에 = 으로 할당시 주의점.
df1[['new_col'] = df2['col'] 하면 concat처럼 눈에보이는 순서를 간직하고 붙는게 아닌, index에 맞게 붙음. 따라서 그냥 = 로 할당해주면 안됨.
df1.reset_index() 활용.

#11 pandas sum  #df.sum의 형태.
df.sum() #세로별합
df.sum(axis = 0) # 세로합

df.sum(axis = 1) #가로합

#12. datetime, 시간 정보 전처리.
df = pd.DataFrame([20201101025616])
pd.to_datetime(df['해당컬럼'],format ='%Y%m%d%H%M%S') # format으로 형태를 지정해줄 수 있음.

df = pd.DataFrame({'Birth':['2019-01-01 09:10:00']}) #이런형태라면 아래처럼
df['Birth'] = pd.to_datetime(df['Birth'], format='%Y-%m-%d %H:%M:%S', errors='raise')

#참고
#https://m.blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221603462366&proxyReferer=https:%2F%2Fwww.google.com%2F

#12 -2 datetime 연산

#1)  pd.to_datetime(df컬럼) #datetime타입으로 바꿔주기 
df['date'] = pd.to_datetime(df['date'])

#2) pd.DateOffset(weeks/months/days = n)으로 연산가능
released_date_code['Released_date'][0] + pd.DateOffset(weeks=1)

https://tariat.tistory.com/631 참조

#12 - 3 datetime pd.read_csv읽어올때 인식도 가능
df = pd.read_csv('diabetes.csv',parse_dates=["date"]) # 가령 이런식으로 date가 있는 컬럼을 지정해줘도 나중에
# 연도 컬럼 전처리
df["year"] = df['date'].dt.year
df["month"] = df['date'].dt.month
df["hour"] = df['date'].dt.hour
# 이런거 가능.


#13. pandas 행 개수 세기
len(Series) #하면 간단히 세기 가능.

#14. merge 
pd.merge(df_left,df_right,on = 'col name',how = 'inner'/'outer/left/right')

#15. apply로 여러 컬럼 값 바꾸기
df['NewCol'] = df.apply(lambda x: segmentMatch(x['TimeCol'], x['ResponseCol']), axis=1)
#이런식으로 df.apply(lambda x: func(x['col1']x['col2']),axis = 1) 로 넣으면 됨. axis = 1 은 row 별로 돈다는 뜻. 

#apply with 하나의 컬럼.
#내가 익숙했던 apply, lambda는 아래와 같았다.
df['new col'] = df['col'].apply(lambda x: func(x)) # 하면 df에서 col의 row를 돌며 func를 적용함. 
#그런데! #15와같이 쓸떄 axis 의 default 는 axis = 0 임. 열별로 도는 것. 따라서 조심해야함. 

#16 1 상위 값 몇개 뽑기. max

#시리즈.nlargest(n, keep='first')

n : 상위 몇개까지 나타낼 것인가
keep='first' : 동등한 값이 있다면 제일 먼저 나온 값을 보여준다.

시리즈의 경우엔 컬럼을 명시할 필요 없다.

#데이터프레임.nlargest(n, columns, keep='first')

데이터프레임의 경우 우선 순위에 따라 컬럼을 명시해 줄 수 있다.

# #16 1 상위 값 몇개 뽑기. min
.smallest(n)

#16 df.isin() #여러개 bool, 색인 쓰고싶으면 df.isin([원소1,원소2])활용!
df.isin(Series) #series도 들어가기 가능

#17 pandas boolean 다루기
df
>>
  0
0	False
1	False
2	False
3	False
4	False
...	...
4732	False
4733	False
4734	False
4735	False
4736	False
4737 rows × 1 columns
이거로 다른 df에서 True인 레코드만 뽑아내기불가.
이유: Series가 아님. 위에 타입은 dataframe임. 
0	NaN
1	NaN
2	NaN
..
막 이렇게 뜰것.

#18. Series에서 평균값 구하기 Series.mean()
Series에게 .average()는 통하지 않음. mean()으로 해야함.
cf) python, numpy
#python --> sum(list)/len(list) # average함수 
#numpy --> array.mean() np.array([1,2,3]).mean() --> 2.0

#19. .count()
 count()는 Null 값이 아닌 행(count Non-null rows)만 세며,
출처: https://rfriend.tistory.com/450 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]

#20. Instead of using double indexing, Use .loc
df['a_col']['b_col'] 해서 자료 접근하면 result는 같음. 그런데 값을 바꾸지는 못함. (이유는 아래)
https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

#21. 행 삽입하는 코드

idx = 9 ## 원하는 인덱스

temp1 = ev_df[ev_df.index < idx]
temp2 = ev_df[ev_df.index >= idx]
ev_df = temp1.append(r12,ignore_index=True).append(temp2, ignore_index=True)

#22. 결측치 있는 행, 열 제거
df.dropna(axis = 0) # 행 제거
df.dropna(axis = 1) # 열 제거

#23. 리스트 포함한 df를 to_csv로 저장하면 안돼. str로 깨짐.
pickle 로 저장하면 됨

# 24. groupby
group = df['출력할col'].groupby(df['그룹지을col'])
group.sum group.mean group.size 모두 

출처: https://rfriend.tistory.com/383 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]
https://rfriend.tistory.com/383

#25. 한번에 df 컬럼들 type 바꾸기
df.astype('float') #모든열
df.astype({'컬럼명':dtype}) #특정열의 data type바꾸기

#26. 실수로 리스트 csv로 저장. string to list
import ast
x = '[ "A","B","C" , "D"]'

x = ast.literal_eval(x)
x #리스트로 잘 바뀜

#27 날짜컬럼 처리
pd.to_datetime(df['date컬럼']) #해당 date 컬럼을 datetime으로 인식

#tip! date컬럼 기준으로 merge하면 date컬럼 인식 다시 해줘야함.
df_merged = pd.merge(df1,df2,on = 'date', how = 'left') #후에 datetime으로 바꿔주기
df_merged['date'] = pd.to_datetime(df_merged['date'])


#28. One-hot Encoding
one_hot_result_1=pd.get_dummies(hd['범주형변수 컬럼']) #N인코딩
one_hot_result_1=pd.get_dummies(hd['범주형변수 컬럼'],drop_first= True) # N-1인코딩

#29. Series를 쉽게 DF로 바꾸는법
df['컬럼']은 물론 series.
df[['컬럼']] 으로하면 df로 탄생 워우!

# 30. tqdm으로 걸릴 시간 예측(feat. apply)
from tqdm import tqdm
tqdm.pandas()
df["col1"].progress_apply(lambda x : x*2) # that was easy

#31 AttributeError: Can't get attribute 'new_block' on <module 'pandas.core.internals.blocks'
이 에러는 판다스 버젼이 안맞는 것 끼리 작업해서 불러오려하면 생기는 에러.
다이렉트로 pd 객체 말고 pickle 통해서 pd객체 옮겨올 때도 발생 할 수 있는 에러.

# 32. df.drop_duplicates()
df.drop_duplicates(['col']) # col에 중복인 행 제거하고 df 보여줌.

#33. 리뷰를 문장으로 쪼개기. (doc2sent mention을 sentence로.)

review_sents = pd.DataFrame(data = [], columns = temp_df.columns)
review_sents


#1) review_sents에 review 기본정보 sents 갯수만큼 추가
def fill_df(row):
  for i in range(len(row['ae_pre'])):
    global review_sents
    #2) sents 배치
    sent = row['ae_pre'][i]
    row['sent'] = sent
    review_sents = review_sents.append(row)
    
 

temp_df.progress_apply(lambda row: fill_df(row),axis = 1)
review_sents

#34 엑셀 작성자들 파일 모으기, 1 표시 두번 이상인 것 

new_df = pd.DataFrame(columns = aspect_terms.columns)
aspects = ['classified1','classified2']
# len(aspects)
for aspect in aspects:
  file_dir = '/gdrive/MyDrive/LG 플젝-민/topic 존재 확인/aspect term 추가/' + aspect
  os.chdir(file_dir)

  aspect_df = pd.DataFrame()

  for file in os.listdir(): # 파일 합치기
    temp = pd.read_excel(file,index_col = 0)
    aspect_df = pd.concat([temp,aspect_df],axis = 0) # 작성자들 df 합치기
  aspect_df = aspect_df[~aspect_df['new_aspect_term'].isna()] # 1표시된거만 남기기
  group = aspect_df['aspect'].groupby(aspect_df['sim_words'])
  new_aspect_terms = list(group.size()[group.size() >= 2].index) # 2번 이상 체크된 sim word 만 추리기
  
  new_df[aspect] = pd.Series(new_aspect_terms)
  print('aspect: ',aspect, new_aspect_terms)


#35. values to column
https://stackoverflow.com/questions/26255671/pandas-column-values-to-columns 참조함.


	source	YearQ	company
0	Reddit	2021_4Q	lg
1	Blog Post	2021_4Q	lg
2	Forums	2021_4Q	lg
3	Forums	2021_4Q	lg
4	Forums	2021_4Q	lg

-->이걸 source 가 컬럼으로 가게하고싶을때는 pivot table 쓰면됨.
가령, source들의 count 값을 컬럼으로 보고싶다면
df.pivot_table(values='company', index=df['YearQ'], columns='source', aggfunc='count')
source	Blog Post	Facebook	Forums	Instagram	Reddit	Reviews	Twitter	Youtube
YearQ								
2019_1Q	312	886	493	35	218	4213	779	56
2019_2Q	230	2107	498	52	216	4874	1167	61
2019_3Q	201	1563	586	275	402	7747	1373	32
2019_4Q	302	2526	475	123	631	3903	909	30
2020_1Q	294	1420	414	138	479	3991	1090	52
2020_2Q	155	2999	516	221	672	4933	1757	30
2020_3Q	396	5035	638	1085	896	4875	9350	38
2020_4Q	278	2066	590	182	724	5289	4680	50
2021_1Q	335	1509	433	144	735	6178	2044	41
2021_2Q	318	2389	583	265	900	8081	1957	65
2021_3Q	370	4212	629	1131	1258	8181	2361	34
2021_4Q	414	2274	520	378	991	6592	1555	29
일케 간단하게 바꿀 수 있다.


