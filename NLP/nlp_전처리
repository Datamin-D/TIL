#1. 표제어 추출 시 특정 단어가 분리될 때 합치는 법

test1 =  ["네","고","왕", "좋다"] # 0,1,2번째 인덱스에 네,고,왕 발견. 잘못 표제어 추출됨.
#test2 = ['나',"네","고","왕", '네' ,"좋다",'고']
tlist = test2
sent = "".join(tlist) # 띄어쓰기없게 합치기
p = re.compile('네고왕')
s = p.search(sent)
a = s.span() # "네고왕"

#리스트의 네고왕 인덱스 저장
start = a[0]
end = a[1]-1
#'네','고','왕' 삭제
del(tlist[start:end])
# 합친 글자 추가 
tlist[start] = '네고왕'
tlist # 정상적으로 리스트 저장됨


#2. (영어) 전처리 함수
import re
from tqdm import tqdm
tqdm.pandas()


## http없애기, 이스케이프시퀀스들 제거, 유니코드 변환
def remove_link_punc_unicode(string):
  # removing links # 정상 작동
  temp_string = re.sub('http[s]?://(?:[a-zA-Z]|[0–9]|[$-_@.&+]|(?:%[0–9a-fA-F][0–9a-fA-F]))+', ' ', string)
  
  # get lost \n \t \v 
  temp_string = re.sub("[\t\n\r\f\v]"," ",temp_string)

  # get lost unicode  
  temp_string = temp_string.encode("ascii", "ignore").decode('utf-8')
  # removing all everything except a-z english letters # 작동
  regex = re.compile('[^a-zA-Z]')
  temp_string = regex.sub(' ', temp_string)

#   removing extra spaces #작동
  clean_string = re.sub(' +', ' ', temp_string).lower()

  return clean_string
  
  

review_sents['remove_link_punc'] = review_sents['sent'].progress_apply(lambda customer_sents: remove_link_punc_unicode(customer_sents)) 

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from tqdm import tqdm
tqdm.pandas()



stop = stopwords.words('english')


### 토큰화해서 preprocessing
def preprocessing(text):   #Preprocessing
    # tokenize into words
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)] 
    
#     remove stopwords
    stop = stopwords.words('english')
    tokens = [token for token in tokens if token not in stop]
#     print(tokens)
    
    # remove words less than three letters
    tokens = [word for word in tokens if len(word) >= 3]
#     print(tokens)
    # lower capitalization
    tokens = [word.lower() for word in tokens]
#     print(tokens)
    # lemmatize
    lmtzr = WordNetLemmatizer()
    tokens = [lmtzr.lemmatize(word) for word in tokens]
#     print(tokens)
    preprocessed_text = tokens
#     preprocessed_text= ' '.join(tokens)

    # Stemming
#     stemmer = PorterStemmer()
#     preprocessed_text = [stemmer.stem(w) for w in preprocessed_text]
    
    return preprocessed_text

review_sents['pre_sent'] = review_sents['remove_link_punc'].progress_apply(lambda customer_sents: preprocessing(customer_sents)) 
